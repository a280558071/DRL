{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Property\n",
    "## Definition\n",
    "A state $S_t$ is *Markov* if and only if \n",
    "$$\\mathbb{P}[S_{t+1}|S_t]=\\mathbb{P}[S_{t+1}|S_t,\\dots,S_t]$$\n",
    "- The state captures all relevant information from the history\n",
    "- Once the state is known, the history may be thrown away\n",
    "- i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "# State Transition Matrix\n",
    "For a Markov state $s$ and successor state $s'$, the *state transition probability* is defined by $$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t=s]$$ \n",
    "State transition matrix $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s'$,\n",
    "$$\\mathcal{P}=\\mathrm{from} \n",
    "\\begin{matrix}\n",
    "    \\mathrm{to}\\\\\n",
    "    \\begin{bmatrix}\n",
    "    \\mathcal{P}_{11},\\dots,\\mathcal{P}_{1n}\\\\\n",
    "    \\vdots,\\ddots&\\vdots\\\\\n",
    "    \\mathcal{P}_{n1}&\\dots&\\mathcal{P}_{nn}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{matrix}$$\n",
    "where each row of the matrix sums to 1.\n",
    "\n",
    "# Markov Process\n",
    "A Markov process is a memory less random process, random process, i.e. a sequence of random states $S_1,S_2,\\cdots$ with the Markov property.\n",
    "## Definition\n",
    "A *Markov Process* (or *Markov Chain*) is a tuple $<\\mathcal{S,P}>$\n",
    "- $\\mathcal{S}$ is a (finite) set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "$$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t=s]$$\n",
    "\n",
    "# Example: Student Markov Chain\n",
    "![ExampleStudentMarkovChain](./pics/ExampleStudentMarkovChain.jpg)\n",
    "Sample episodes for Student Markov Chain starting from $S_1=C1$\n",
    "$$S_1,S_2,\\cdots,S_T$$\n",
    "- C1,C2,C3,Pass,Sleep\n",
    "- C1,FB,FB,C1,C2,Sleep\n",
    "- C1,C2,C3,Pub,C2,C3,Pass,Sleep\n",
    "- C1,FB,FB,C1,C2,C3,Pub,C1,FB,FB,FB,C1,C2,C3,Pub,C2,Sleep\n",
    "$$\\begin{array}{lllllll}\n",
    "{C1} & {C2} & {C3} & {Pass} & {Pub} & {FB} & {Sleep}\n",
    "\\end{array}$$\n",
    "$$\\mathcal{P}=\\begin{bmatrix}\n",
    "0 & 0.5 & 0 & 0 & 0 & 0.5 & 0\\\\\n",
    "0 & 0 & 0.8 & 0 & 0 & 0 & 0.2\\\\\n",
    "0 & 0 & 0 & 0.6 & 0.4 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
    "0.2 & 0.4 & 0.4 & 0 & 0 & 0 & 0 \\\\\n",
    "0.1 & 0 & 0 & 0 & 0 & 0.9 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state transition matrix (P) of Student Markov Chain is:\n",
      "[[0.  0.5 0.  0.  0.  0.5 0. ]\n",
      " [0.  0.  0.8 0.  0.  0.  0.2]\n",
      " [0.  0.  0.  0.6 0.4 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1. ]\n",
      " [0.2 0.4 0.4 0.  0.  0.  0. ]\n",
      " [0.1 0.  0.  0.  0.  0.9 0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Markov Property\n",
    "# State Transition Matrix\n",
    "# Example: Student Markov Chain\n",
    "import numpy as np\n",
    "# {C1} & {C2} & {C3} & {Pass} & {Pub} & {FB} & {Sleep}\n",
    "P=np.matrix([[0 , 0.5 , 0 , 0 , 0 , 0.5 , 0],\n",
    "[0 , 0 , 0.8 , 0 , 0 , 0 , 0.2],\n",
    "[0 , 0 , 0 , 0.6 , 0.4 , 0 , 0],\n",
    "[0 , 0 , 0 , 0 , 0 , 0 , 1],\n",
    "[0.2 , 0.4 , 0.4 , 0 , 0 , 0 , 0],\n",
    "[0.1 , 0 , 0 , 0 , 0 , 0.9 , 0],\n",
    "[0 , 0 , 0 , 0 , 0 , 0 , 1]])\n",
    "print(\"The state transition matrix (P) of Student Markov Chain is:\")\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process\n",
    "A Markov reward process is a Markov chain with values.\n",
    "## Definition\n",
    "A *Markov Reward Process* is a tuple $<\\mathcal{S,P,{\\color{red}R,\\gamma}\n",
    "}>$\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "- ${\\color{red}\\mathcal{R}}$ is a reward function, ${\\color{red}\\mathcal{R_s}=\\mathbb{E}[R_{t+1}|S_{t}=s]}$\n",
    "- ${\\color{red}\\gamma}$ is a discount factor, ${\\color{red}\\gamma\\in[0,1]}$\n",
    "## Example:\n",
    "![ExampleStudentMarkovChain-MDP](./pics/ExampleStudentMarkovChain-MRP.jpg)\n",
    "## Return\n",
    "### Definition\n",
    "The *return* $G_t$ is the total discounted reward from time-step $t$.\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}{\\gamma^kR_{t+k+1}}$$\n",
    "- The *discount* $\\gamma\\in[0,1]$ is the present value of future rewards\n",
    "- The value of receiving reward $R$ after $k+1$ time-steps is $\\gamma^kR$\n",
    "- This value immediate reward above delayed reward.\n",
    "  - $\\gamma$ close to 0 leads to \"myopic\" evaluation\n",
    "  - $\\gamma$ close to 1 leads to \"far-sighted\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return of  ['C1', 'C2', 'C3', 'Pass', 'Sleep'] : -2.25\n",
      "Return of  ['C1', 'FB', 'FB', 'C1', 'C2', 'Sleep'] : -3.125\n",
      "Return of  ['C1', 'C2', 'C3', 'Pub', 'C2', 'C3', 'Pass', 'Sleep'] : -3.40625\n",
      "Return of  ['C1', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C1', 'FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C2', 'Sleep'] : -3.196044921875\n"
     ]
    }
   ],
   "source": [
    "# Markov Reward Process\n",
    "# A Markov reward process is a Markov chain with values.\n",
    "## Return\n",
    "### Definition\n",
    "# The *return* $G_t$ is the total discounted reward from time-step $t$.\n",
    "### Example: Student MRP Returns\n",
    "gamma=0.5\n",
    "# C1: C2: C3: Pass: Pub: FB: Sleep\n",
    "R_s=np.array([-2,-2,-2,10,1,-1,0])\n",
    "# C1,C2,C3,Pass,Sleep:\n",
    "G1= R_s[0]+gamma*R_s[1]+gamma**2*R_s[2]+gamma**3*R_s[3]\n",
    "\n",
    "# Solve it by define a function\n",
    "R_dict={'C1':-2, 'C2':-2, 'C3':-2, 'Pass':10,'Pub':1,'FB':-1,'Sleep':0} # Reward dict\n",
    "Route=[['C1','C2','C3','Pass','Sleep'], # Route list 1\n",
    "['C1','FB','FB','C1','C2','Sleep'], # Route list 2\n",
    "['C1','C2','C3','Pub','C2','C3','Pass','Sleep'], # Route list 3\n",
    "['C1','FB','FB','C1','C2','C3','Pub','C1','FB','FB','FB','C1','C2','C3','Pub','C2','Sleep']] # Route list 4\n",
    "\n",
    "def MRPReturn(Route,R_dict):\n",
    "    \"This fucntion calculate the Return based on Route and Reward dictionary\"\n",
    "    Return = 0\n",
    "    for i in range(len(Route)):\n",
    "        Return = Return + gamma**(i)*R_dict[Route[i]]\n",
    "    return Return\n",
    "\n",
    "Return=np.zeros(len(Route))\n",
    "for i in range(len(Route)):\n",
    "    Return[i]=MRPReturn(Route[i],R_dict)\n",
    "\n",
    "# Print them\n",
    "for i in range(len(Route)):\n",
    "    print(\"Return of \",Route[i],\":\",Return[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function\n",
    "The value function $v(s)$ gives the long-term value of state $s$\n",
    "### Definition\n",
    "The *state valuefunction* $v(s)$ of an MRP is the expected *return* ($G_t$) starting from state $s$\n",
    "$$v(s)=\\mathbb{E}[G_t|S_t=s]$$\n",
    "### Example: Student MRP Returns\n",
    "Sample <font color=Red>returns</font> for Student MRP:\n",
    "Starting from $S_1=C1$ with $\\gamma=\\frac{1}{2}$\n",
    "$$G_1=R_2+\\gamma R_3+\\dots+\\gamma^{T-2}R_T$$\n",
    "\n",
    "- C1,C2,C3,Pass,Sleep:$$G_1=-2+\\frac{1}{2}\\times(-2)+\\frac{1}{4}\\times(-2)+\\frac{1}{8}\\times10=-2.25$$\n",
    "- C1,FB,FB,C1,C2,Sleep:$$G_1=-2+\\frac{1}{2}\\times(-1)+\\frac{1}{4}\\times(-1)+\\frac{1}{8}\\times(-2)+\\frac{1}{16}\\times(-2)=-3.125$$\n",
    "- C1,C2,C3,Pub,C2,C3,Pass,Sleep:...\n",
    "- C1,FB,FB,C1,C2,C3,Pub,C1,FB,FB,FB,C1,C2,C3,Pub,C2,Sleep:...\n",
    "\n",
    "### Example: State-Value Function for Student MRP(1)\n",
    "![ExampleStudentMarkovChain-MDP-SVF](./pics/ExampleStudentMarkovChain-MRP-State-value-function1.jpg)\n",
    "### Example: State-Value Function for Student MRP(1)\n",
    "![ExampleStudentMarkovChain-MDP-SVF](./pics/ExampleStudentMarkovChain-MRP-State-value-function2.jpg)\n",
    "### Example: State-Value Function for Student MRP(1)\n",
    "![ExampleStudentMarkovChain-MDP-SVF](./pics/ExampleStudentMarkovChain-MRP-State-value-function3.jpg)\n",
    "\n",
    "## Bellman Equation for MRPs\n",
    "The value function can be decomposed into two parts:\n",
    "- immediate reward $R_{t+1}$\n",
    "- discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "$$\n",
    "\\begin{align}\n",
    "v(s) & =\\mathbb{E}[G_t|S_t=s]\\\\\n",
    "& =\\mathbb{E}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+\\dots|S_t=s]\\\\\n",
    "& =\\mathbb{E}[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}+\\dots)|S_t=s]\\\\\n",
    "& =\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|S_t=s]\\\\\n",
    "& =\\mathbb{E}[R_{t+1}+\\gamma v(S_{t+1})|S_t=s]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "v(s)=\\mathcal{R_s}+\\gamma \\sum_{s'\\in\\mathcal{S}}{\\mathcal{P_{ss'}}v(s')}\n",
    "$$\n",
    "\n",
    "### Example:Bellman Equation for Student MRP\n",
    "![Bellman](./pics/BellmanEquationExample.jpg)\n",
    "### Bellman Equation in Matrix Form\n",
    "The Bellman equation can be expressed concisely using matrices,\n",
    "$$\\mathbf{v}=\\mathbf{\\mathcal{R}}+\\gamma \\mathcal{P}\\mathbf{v}$$\n",
    "where $\\mathbf{v}$ is a column vector with one entry per state\n",
    "$$\\begin{bmatrix}v(1)\\\\\\vdots\\\\v(n)\\end{bmatrix}=\\begin{bmatrix}\\mathcal{R}_1\\\\\\vdots\\\\\\mathcal{R}_n\\end{bmatrix}+\\gamma \\begin{bmatrix}\\mathcal{P}_{11}&\\dots&\\mathcal{P}_{1n}\\\\\\vdots\\\\\\mathcal{P}_{n1}&\\dots&\\mathcal{P}_{nn}\\end{bmatrix}\\begin{bmatrix}v(1)\\\\\\vdots\\\\v(n)\\end{bmatrix}$$\n",
    "### Solving the Bellman Equation\n",
    "- The Bellman equation is a linear equation \n",
    "- It can be solved directly:\n",
    "$$\n",
    "\\mathbf{v}=\\mathbf{\\mathcal{R}}+\\gamma \\mathcal{P}\\mathbf{v}→\n",
    "\\mathbf{v}=(I-\\gamma \\mathcal{P})^{-1}\\mathbf{\\mathcal{R}}\n",
    "$$\n",
    "- Computational complexity is $O(n^3)$ for $n$ states\n",
    "- Direct solution only possible for small MRPs\n",
    "- There are many iterative methods for large MRPs, e.g.\n",
    "  - Dynamic programming\n",
    "  - Monte-Carlo evaluation\n",
    "  - Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of State C3:  4.3\n",
      "Value of State C3 calculated by Bellman Equation (Gamma=1):  4.32\n",
      "When Gamma is  0.999999  :\n",
      "[[-12.54296219]\n",
      " [  1.4568013 ]\n",
      " [  4.32100594]\n",
      " [ 10.        ]\n",
      " [  0.80253065]\n",
      " [-22.54274676]\n",
      " [  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "## Value Function\n",
    "# The value function $v(s)$ gives the long-term value of state $s$\n",
    "### Definition\n",
    "# The *state valuefunction* $v(s)$ of an MRP is the expected *return* ($G_t$) starting from state $s$\n",
    "\n",
    "# Solve it by define a function\n",
    "Value_dict={'C1':-13, 'C2':1.5, 'C3':4.3, 'Pass':10,'Pub':0.8,'FB':-23,'Sleep':0} # Value function dict\n",
    "print(\"Value of State C3: \",Value_dict['C3'])\n",
    "print(\"Value of State C3 calculated by Bellman Equation (Gamma=1): \", R_dict['C3']+P[2,3]*Value_dict['Pass']+P[2,4]*Value_dict['Pub'])\n",
    "\n",
    "## Bellman Equation for MRPs\n",
    "### Example:Bellman Equation for Student MRP\n",
    "l=len(Value_dict)\n",
    "\n",
    "R_array = np.array(list(R_dict.values()))\n",
    "R_array = np.array([R_array])\n",
    "\n",
    "# print(np.linalg.inv(np.identity(l)-gamma*P))\n",
    "gamma2 = 0.999999 # when gamma2=1, the inverse matrix is singular, WHY???\n",
    "V=np.linalg.inv(np.identity(l)-gamma2*P)*(R_array.T)\n",
    "print(\"When Gamma is \",gamma2,\" :\")\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "A Markov decision process (MDP) is a Markov reward process with decisions. It is an *environment* in which all states are Markov.\n",
    "## Definition\n",
    "A *Markov Decision Process* is tuple $<\\mathcal{S},{\\color{red}\\mathcal{A}},\\mathcal{P},\\mathcal{R},\\mathcal{\\gamma}>$\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- <font color=Red> $\\mathcal{A}$ is a finite set of actitions</font>\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "  $$\\mathcal{P}_{ss'}^{\\color{red}a}=\\mathbb{P}[S_{t+1}=s'|S_t=s,A_t={\\color{red}a}]$$\n",
    "- $\\mathcal{R}$ is a reward function, $\\mathcal{R}_s^{\\color{red}a}=\\mathbb{E}[R_{t+1}|S_t=s,A_t={\\color{red}a}]$\n",
    "- $\\gamma$ is a discount factor $\\gamma\\in[0,1]$\n",
    "\n",
    "## Example: Student MDP\n",
    "![ExampleStudentMarkovDecisionProcess](./pics/ExampleStudentMarkovDecisionProcess.jpg)\n",
    "\n",
    "## Policies\n",
    "### Definition\n",
    "A *policy* $\\pi$ is a distribution over actions given states,\n",
    "$$\\pi(a|s)=\\mathbb{P}[A_t=a|S_t=s]$$\n",
    "- A policy fully defines the behaviour of an agent\n",
    "- MDP policies depend on the current state (not the history)\n",
    "- i.e. Policies are *stationary* (time-independent),\n",
    "$$A_t\\sim\\pi(·|S_t),\\forall t>0 $$\n",
    "- Given an MDP $M=\\langle \\mathcal{S,A,P,R,\\gamma} \\rangle$ and a policy $\\pi$\n",
    "- The state sequnce $S_1,S_2,\\dots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}}\\rangle$\n",
    "- The state and reward sequnce $S_1,R_2,S_2,\\dots$ is a Markov reward process $\\langle \\mathcal{S,P^{\\pi},R^{\\pi},\\gamma} \\rangle$\n",
    "- where\n",
    "$$\\mathcal{P}^{\\pi}_{s,s'}=\\sum_{a\\in\\mathcal{A}}{\\pi(a|s)\\mathcal{P}^{a}_{s,s'}}\\\\\\mathcal{R}^{\\pi}_{s}=\\sum_{a\\in\\mathcal{A}}{\\pi(a|s)\\mathcal{R}^{a}_{s}}$$\n",
    "\n",
    "## Value Function\n",
    "### Definition\n",
    "The ***state-value function*** $v_\\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$\n",
    "$$v_\\pi(s)=\\mathbb{E}_\\pi[G_t|S_t=s]$$\n",
    "### Definition\n",
    "The ***action-value function*** $q_\\pi(s,a)$ of an MDP is the expected return starting from state $s$, taking action $a$ and then following policy $\\pi$\n",
    "$$q_\\pi(s,a)=\\mathbb{E}_\\pi[G_t|S_t=s,A_t=a]$$\n",
    "### Example: State-Value Function for Student MDP\n",
    "![ExampleStudentMarkovDecisionProcess_SVF](./pics/ExampleStudentMarkovDecisionProcess_StateValueFunction.jpg)\n",
    "\n",
    "## Bellman Expectation Equation\n",
    "The state-value function can again be decomposed into immediate reward plus discounted value of successor state,\n",
    "$$v_\\pi(s)=\\mathbb{E}_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t+1}|S_t=s)]$$\n",
    "The action-value function can similarly be decomposed,\n",
    "$$q_\\pi(s,a)=\\mathbb{E}_\\pi[R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1}|S_t=s,A_t=a)]$$\n",
    "### Bellman Expectation Equation for $V_\\pi,Q_\\pi$\n",
    "$$\n",
    "v_\\pi(s)=\\sum_{a\\in\\mathcal{A}}{\\pi(a|s)q_\\pi(s,a)}\\\\\n",
    "q_\\pi(s,a)=\\mathcal{R}_s^a+\\gamma \\sum_{s'\\in\\mathcal{S}}{\\mathcal{P}^{a}_{ss'}v_\\pi(s')}\\\\\n",
    "v_\\pi(s)=\\sum_{a\\in\\mathcal{A}}{\\pi(a|s)(\\mathcal{R}_s^a+\\gamma \\sum_{s'\\in\\mathcal{S}}{\\mathcal{P}^{a}_{ss'}v_\\pi(s')})}\\\\\n",
    "q_\\pi(s,a)=\\mathcal{R}_s^a+\\gamma \\sum_{s'\\in\\mathcal{S}}{\\mathcal{P}^{a}_{ss'}\\sum_{a'\\in\\mathcal{A}}{\\pi(a'|s')q_\\pi(s',a')}}\\\\\n",
    "$$\n",
    "### Example: Bellman Expectation Equation in Student MDP\n",
    "![ExampleStudentMarkovDecisionProcessSVF_BellmanExpEq](./pics/ExampleStudentMarkovDecisionProcess_StateValueFunction_BellmanExpectationEq.jpg)\n",
    "### Bellman Expectation Equation (Matrix Form)\n",
    "The Bellman expectation equation can be expressed concisely using the induced MRP,\n",
    "$$v_{\\pi} =\\mathcal{R^\\pi+\\gamma P^\\pi v_\\pi}$$\n",
    "with direct solution\n",
    "$$v_{\\pi} =(I-\\mathcal{\\gamma P^\\pi})^{-1}\\mathcal{R^\\pi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4\n",
      "State Value of \"C1\" according to Bellman Expectation Equation:\n",
      "7.390000000000001\n",
      "When Gamma is  1  :\n",
      "[[ -5.]\n",
      " [ -3.]\n",
      " [ 18.]\n",
      " [-16.]\n",
      " [  6.]]\n"
     ]
    }
   ],
   "source": [
    "# Markov Decision Process\n",
    "## Policies\n",
    "## Value Function: state-value function and action-value function\n",
    "## Example: Student MDP\n",
    "# v_π(s) for π(a|s)=0.5, γ =1\n",
    "π_as=0.5\n",
    "gamma3=1\n",
    "Vpi={'C1':-1.3, 'C2':2.7, 'C3':7.4, 'Sleep':0,'FB':-2.3} # State-Value function dict\n",
    "# 'C1', 'C2', 'C3', 'Sleep','FB'\n",
    " # C1→Study→C2: 0.5, C1→FB(action)→FB(state): 0.5\n",
    "Ppi_MDP=np.matrix([[0 , π_as , 0 , 0 , 1-π_as], \\\n",
    " # C2→Study(action)→C3: 0.5, C2→Sleep(action)→Sleep(state): 0.5\n",
    "[0 , 0 , π_as , 1-π_as , 0], \\\n",
    "# C3→Study(action)→Sleep(state): 0.5, C3→Pub(action)→C1: 0.5*0.2, C3→Pub(action)→C2/C3: 0.5*0.4\n",
    "[π_as*0.2 , π_as*0.4 , π_as*0.4 , π_as, 0], \\\n",
    " # Sleep is the end state\n",
    "[0 , 0 , 0 , 0 , 0], \\\n",
    "# FB(state)→FB(action)→FB(state): 0.5, FB(state)→Quit→FB(state): 0.5\n",
    "[π_as , 0 , 0, 0, 1-π_as]]) \n",
    "R_sa={'C1Study':-2,'C1FB':-1,'FBFB':-1,'FBC1':0,'C2Study':-2,'C2Sleep':0,'C3Study':10,'C3Pub':1} #Reward dict\n",
    "R_spi=np.array([π_as*R_sa['C1Study']+(1-π_as)*R_sa['C1FB'], \\\n",
    "    π_as*R_sa['C2Study']+(1-π_as)*R_sa['C2Sleep'], \\\n",
    "    π_as*R_sa['C3Study']+(1-π_as)*R_sa['C3Pub'], \\\n",
    "    0, \\\n",
    "    π_as*R_sa['FBC1']+(1-π_as)*R_sa['FBFB']])\n",
    "\n",
    "### Example: Bellman Expectation Equation in Student MDP\n",
    "print(Vpi['C3'])\n",
    "print('State Value of \"C1\" according to Bellman Expectation Equation:')\n",
    "v_C3=π_as*(R_sa['C3Study']+R_sa['C3Pub']+gamma3*(0.2*Vpi['C1']+0.4*Vpi['C2']+0.4*Vpi['C3']))\n",
    "print(v_C3)\n",
    "\n",
    "### Example: Bellman Expectation Equation (Matrix Form) (not confirmed yet)\n",
    "gamma3 = 1\n",
    "R_spi=np.array([R_spi])\n",
    "l=len(R_spi)\n",
    "Vpi_value=np.linalg.inv(np.identity(l)-gamma3*Ppi_MDP)*(R_spi.T)\n",
    "print(\"When Gamma is \",gamma3,\" :\")\n",
    "print(Vpi_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Value Fuction\n",
    "## Definition\n",
    "The *optimal state-value funtion* $v_*(s)$ is the maximum value function over all policies\n",
    "$$v_*(s)=\\max_{\\pi} v_\\pi(s)$$\n",
    "The *optimal action-value function* $q_*(s,a)$ is the maximum action-value function over all policies\n",
    "$$q_*(s,a)=\\max_{\\pi} q_*(s,a)$$\n",
    "- The optimal value function specifies the best possible performance in the MDP.\n",
    "- An MDP is \"solved\" when we know the optimal value fn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ecfabb7306a3c4c2d025a7ad317103a8bab66622e393fe13dbaf2cd206c6383"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

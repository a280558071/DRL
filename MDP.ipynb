{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Property\n",
    "## Definition\n",
    "A state $S_t$ is *Markov* if and only if \n",
    "$$\\mathbb{P}[S_{t+1}|S_t]=\\mathbb{P}[S_{t+1}|S_t,...,S_t]$$\n",
    "- The state captures all relevant information from the history\n",
    "- Once the state is known, the history may be thrown away\n",
    "- i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "# State Transition Matrix\n",
    "For a Markov state $s$ and successor state $s'$, the *state transition probability* is defined by $$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t=s]$$ $\\vec{x}\\stackrel{\\mathrm{def}}{=}{x_1,\\dots,x_n}$\n",
    "State transition matrix $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s'$,\n",
    "$$\\mathcal{P}=\\mathrm{from} \n",
    "\\begin{matrix}\n",
    "    \\mathrm{to}\\\\\n",
    "    \\begin{bmatrix}\n",
    "    \\mathcal{P}_{11}&\\cdots&\\mathcal{P}_{1n}\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    \\mathcal{P}_{n1}&\\cdots&\\mathcal{P}_{nn}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{matrix}$$\n",
    "where each row of the matrix sums to 1.\n",
    "\n",
    "# Markov Process\n",
    "A Markov process is a memory less random process, random process, i.e. a sequence of random states $S_1,S_2,\\cdots$ with the Markov property.\n",
    "## Definition\n",
    "A *Markov Process* (or *Markov Chain*) is a tuple $<\\mathcal{S,P}>$\n",
    "- $\\mathcal{S}$ is a (finite) set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "$$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t=s]$$\n",
    "\n",
    "# Example: Student Markov Chain\n",
    "![ExampleStudentMarkovChain](./pics/ExampleStudentMarkovChain.jpg)\n",
    "Sample episodes for Student Markov Chain starting from $S_1=C1$\n",
    "$$S_1,S_2,\\cdots,S_T$$\n",
    "- C1,C2,C3,Pass,Sleep\n",
    "- C1,FB,FB,C1,C2,Sleep\n",
    "- C1,C2,C3,Pub,C2,C3,Pass,Sleep\n",
    "- C1,FB,FB,C1,C2,C3,Pub,C1,FB,FB,FB,C1,C2,C3,Pub,C2,Sleep\n",
    "$$\\begin{array}{lllllll}\n",
    "{C1} & {C2} & {C3} & {Pass} & {Pub} & {FB} & {Sleep}\n",
    "\\end{array}$$\n",
    "$$\\mathcal{P}=\\begin{bmatrix}\n",
    "0 & 0.5 & 0 & 0 & 0 & 0.5 & 0\\\\\n",
    "0 & 0 & 0.8 & 0 & 0 & 0 & 0.2\\\\\n",
    "0 & 0 & 0 & 0.6 & 0.4 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
    "0.2 & 0.4 & 0.4 & 0 & 0 & 0 & 0 \\\\\n",
    "0.1 & 0 & 0 & 0 & 0 & 0.9 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "# Markov Reward Process\n",
    "A Markov reward process is a Markov chain with values.\n",
    "## Definition\n",
    "A *Markov Reward Process* is a tuple $<\\mathcal{S,P,{\\color{red}R,\\gamma}\n",
    "}>$\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "- ${\\color{red}\\mathcal{R}}$ is areward function, ${\\color{red}\\mathcal{R_s}=\\mathbb{E}[R_{t+1}|S_{t}=s]}$\n",
    "- ${\\color{red}\\gamma}$ is a discount factor, ${\\color{red}\\gamma\\in[0,1]}$\n",
    "## Example:\n",
    "![ExampleStudentMarkovChain](./pics/ExampleStudentMarkovChain-MDP.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f1ca5308371198f3be646792d3a74adda0590dad69211c8b46b5d002a01f23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

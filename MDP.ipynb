{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Property\n",
    "## Definition\n",
    "A state $S_t$ is *Markov* if and only if \n",
    "$$\\mathbb{P}[S_{t+1}|S_t]=\\mathbb{P}[S_{t+1}|S_t,\\dots,S_t]$$\n",
    "- The state captures all relevant information from the history\n",
    "- Once the state is known, the history may be thrown away\n",
    "- i.e. The state is a sufficient statistic of the future\n",
    "\n",
    "# State Transition Matrix\n",
    "For a Markov state $s$ and successor state $s'$, the *state transition probability* is defined by $$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t=s]$$ \n",
    "State transition matrix $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s'$,\n",
    "$$\\mathcal{P}=\\mathrm{from} \n",
    "\\begin{matrix}\n",
    "    \\mathrm{to}\\\\\n",
    "    \\begin{bmatrix}\n",
    "    \\mathcal{P}_{11}&\\dots&\\mathcal{P}_{1n}\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    \\mathcal{P}_{n1}&\\dots&\\mathcal{P}_{nn}\\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{matrix}$$\n",
    "where each row of the matrix sums to 1.\n",
    "\n",
    "# Markov Process\n",
    "A Markov process is a memory less random process, random process, i.e. a sequence of random states $S_1,S_2,\\cdots$ with the Markov property.\n",
    "## Definition\n",
    "A *Markov Process* (or *Markov Chain*) is a tuple $<\\mathcal{S,P}>$\n",
    "- $\\mathcal{S}$ is a (finite) set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "$$\\mathcal{P}_{ss'}=\\mathbb{P}[S_{t+1}=s'|S_t=s]$$\n",
    "\n",
    "# Example: Student Markov Chain\n",
    "![ExampleStudentMarkovChain](./pics/ExampleStudentMarkovChain.jpg)\n",
    "Sample episodes for Student Markov Chain starting from $S_1=C1$\n",
    "$$S_1,S_2,\\cdots,S_T$$\n",
    "- C1,C2,C3,Pass,Sleep\n",
    "- C1,FB,FB,C1,C2,Sleep\n",
    "- C1,C2,C3,Pub,C2,C3,Pass,Sleep\n",
    "- C1,FB,FB,C1,C2,C3,Pub,C1,FB,FB,FB,C1,C2,C3,Pub,C2,Sleep\n",
    "$$\\begin{array}{lllllll}\n",
    "{C1} & {C2} & {C3} & {Pass} & {Pub} & {FB} & {Sleep}\n",
    "\\end{array}$$\n",
    "$$\\mathcal{P}=\\begin{bmatrix}\n",
    "0 & 0.5 & 0 & 0 & 0 & 0.5 & 0\\\\\n",
    "0 & 0 & 0.8 & 0 & 0 & 0 & 0.2\\\\\n",
    "0 & 0 & 0 & 0.6 & 0.4 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
    "0.2 & 0.4 & 0.4 & 0 & 0 & 0 & 0 \\\\\n",
    "0.1 & 0 & 0 & 0 & 0 & 0.9 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "# Markov Reward Process\n",
    "A Markov reward process is a Markov chain with values.\n",
    "## Definition\n",
    "A *Markov Reward Process* is a tuple $<\\mathcal{S,P,{\\color{red}R,\\gamma}\n",
    "}>$\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix,\n",
    "- ${\\color{red}\\mathcal{R}}$ is areward function, ${\\color{red}\\mathcal{R_s}=\\mathbb{E}[R_{t+1}|S_{t}=s]}$\n",
    "- ${\\color{red}\\gamma}$ is a discount factor, ${\\color{red}\\gamma\\in[0,1]}$\n",
    "## Example:\n",
    "![ExampleStudentMarkovChain-MDP](./pics/ExampleStudentMarkovChain-MRP.jpg)\n",
    "## Return\n",
    "### Definition\n",
    "The *return* $G_t$ is the total discounted reward from time-step $t$.\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}{\\gamma^kR_{t+k+1}}$$\n",
    "- The *discount* $\\gamma\\in[0,1]$ is the present value of future rewards\n",
    "- The value of receiving reward $R$ after $k+1$ time-steps is $\\gamma^kR$\n",
    "- This value immediate reward above delayed reward.\n",
    "  - $\\gamma$ close to 0 leads to \"myopic\" evaluation\n",
    "  - $\\gamma$ close to 1 leads to \"far-sighted\" evaluation\n",
    "\n",
    "## Value Function\n",
    "The value function $v(s)$ gives the long-term value of state $s$\n",
    "### Definition\n",
    "The *state valuefunction* $v(s)$ of an MRP is the expected *return* ($G_t$) starting from state $s$\n",
    "$$v(s)=\\mathbb{E}[G_t|S_t=s]$$\n",
    "### Example: Student MRP Returns\n",
    "Sample <font color=Red>returns</font> for Student MRP:\n",
    "Starting from $S_1=C1$ with $\\gamma=\\frac{1}{2}$\n",
    "$$G_1=R_2+\\gamma R_3+\\dots+\\gamma^{T-2}R_T$$\n",
    "\n",
    "- C1,C2,C3,Pass,Sleep:$$G_1=-2+\\frac{1}{2}\\times(-2)+\\frac{1}{4}\\times(-2)+\\frac{1}{8}\\times10=2.25$$\n",
    "- C1,FB,FB,C1,C2,Sleep:$$G_1=-2+\\frac{1}{2}\\times(-1)+\\frac{1}{4}\\times(-1)+\\frac{1}{8}\\times(-2)+\\frac{1}{16}\\times(-2)=-3.125$$\n",
    "- C1,C2,C3,Pub,C2,C3,Pass,Sleep:...\n",
    "- C1,FB,FB,C1,C2,C3,Pub,C1,FB,FB,FB,C1,C2,C3,Pub,C2,Sleep:...\n",
    "\n",
    "### Example: State-Value Function for Student MRP(1)\n",
    "![ExampleStudentMarkovChain-MDP-SVF](./pics/ExampleStudentMarkovChain-MRP-State-value-function1.jpg)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f1ca5308371198f3be646792d3a74adda0590dad69211c8b46b5d002a01f23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
